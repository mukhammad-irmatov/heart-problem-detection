# -*- coding: utf-8 -*-
"""Detecting_heart_disease.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1gwlOdLa6lcrixQvfDcQH1t2ZxXZr5SAF

<h1> Heart disease prediction </h1>
<p>I obtained the dataset for this machine learning project from Kaggle (https://www.kaggle.com/ronitf/heart-disease-uci) and will use Machine Learning to predict whether or not a person is suffering from Heart Disease.</p>

<h2>Importing necessary libraries </h2>
Let's start by importing all of the required libraries. To begin, I'll use numpy and pandas. I'll use the matplotlib pyplot subpackage for visualization, rcParams for plot formatting, and rainbow for colors. The sklearn library will be used to create Machine Learning models and data processing.
"""

# Commented out IPython magic to ensure Python compatibility.
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from matplotlib import rcParams
from matplotlib.cm import rainbow
# %matplotlib inline
import warnings
warnings.filterwarnings('ignore')

"""I'll import a several libraries to process the data. I'll use the train test split function to split the available dataset for testing and training. I'm using StandardScaler to scale the features.

Next, I'll import all the Machine Learning algorithms I will be using.

1.   K Neighbors Classifier
2.   Support Vector Classifier
3.   Decision Tree Classifier
4.   Random Forest Classifier
"""

from sklearn.neighbors import KNeighborsClassifier
from sklearn.svm import SVC
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier

"""<h3>Import dataset</h3>
Now that we have all of the necessary libraries, I can import the dataset and examine it. The data is saved in the dataset.csv file. To read the dataset, I'll use the pandas read csv function.
"""

data = pd.read_csv('dataset.csv')

"""Now that we have all of the necessary libraries, I can import the dataset and examine it. The data is saved in the data.csv file. To read the dataset, I'll use the pandas read csv function."""

data.info()

"""There are no missing values in the dataset, which seems to have 303 rows. There are a total of 13 characteristics and one target value that we need to locate.


"""

data.describe()

"""Each feature column has a distinct scale and is highly variable. While the age limit is set at 77, the chol (serum cholestoral) limit is set at 564.

<h2>Understanding the data </h2>
Now we may utilize visuals to better comprehend our data before considering any processing options.
"""

rcParams['figure.figsize'] = 20, 14
plt.matshow(data.corr())
plt.yticks(np.arange(data.shape[1]), data.columns)
plt.xticks(np.arange(data.shape[1]), data.columns)
plt.colorbar()

"""Looking at the correlation matrix above, it's clear that certain attributes have a negative connection with the goal value, whilst others have a positive association. The histograms for each variable will be examined next."""

data.hist()

"""Looking at the histograms above, I can see that each characteristic has its own distribution range. As a result, scaling before our projections should be quite useful. The categorical characteristics also stand out.

<h5>Working with a dataset with goal classes that are about similar in size is always a smart idea. Let us investigate this further.</h5>
"""

rcParams['figure.figsize'] = 8,6
plt.bar(data['target'].unique(), data['target'].value_counts(), color = ['red', 'green'])
plt.xticks([0, 1])
plt.title('Count of each Target Class')
plt.ylabel('Count')
plt.xlabel('Target Classes')

"""Although the two classes are not precisely 50 percent each, the ratio is excellent enough to allow us to continue without losing or expanding our data.

<h3> Data Processing</h3>
Before training the Machine Learning models, I discovered that I needed to turn certain category variables into dummy variables and scale all of the values. To begin, I'll generate dummy columns for category variables using the get_dummies method.
"""

dataset = pd.get_dummies(data, columns = ['sex', 'cp', 'fbs', 'restecg', 'exang', 'slope', 'ca', 'thal'])

"""To scale my dataset, I'll use the StandardScaler from sklearn."""

standardScaler = StandardScaler()
columns_to_scale = ['age', 'trestbps', 'chol', 'thalach', 'oldpeak']
dataset[columns_to_scale] = standardScaler.fit_transform(dataset[columns_to_scale])

"""The data is not ready for our Machine Learning application..

<h2> Machine learning part </h2>
To divide our dataset into training and testing datasets, I'll import train test split. Then, to train and test the data, I'll import all of the Machine Learning models I'll be utilizing.
"""

y = dataset['target']
X = dataset.drop(['target'], axis = 1)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.33, random_state = 0)

"""<h2> K Neighbors Classifier algorithm </h2>
The classification score varies based on different values of neighbors that we choose. Thus, I'll plot a score graph for different values of K (neighbors) and check when do I achieve the best score.
"""

knn_scores = []
for k in range(1,21):
    knn_classifier = KNeighborsClassifier(n_neighbors = k)
    knn_classifier.fit(X_train, y_train)
    knn_scores.append(knn_classifier.score(X_test, y_test))

"""In the array knn scores, I have the scores for various neighbor values. Now I'll plot it to determine which value of K yielded the greatest results."""

plt.plot([k for k in range(1, 21)], knn_scores, color = 'red')
for i in range(1,21):
    plt.text(i, knn_scores[i-1], (i, knn_scores[i-1]))
plt.xticks([i for i in range(1, 21)])
plt.xlabel('Number of Neighbors (K)')
plt.ylabel('Scores')
plt.title('K Neighbors Classifier scores for different K values')

"""From the plot above, it is clear that the maximum score achieved was 0.87 for the 8 neighbors.


"""

print("The score for K Neighbors Classifier is {}% with {} nieghbors.".format(knn_scores[7]*100, 8))

"""# Support vector machine 
Support Vector Classifier has a number of kernels. I'll put some of them to the test and see who comes out on top.
"""

svc_scores = []
kernels = ['linear', 'poly', 'rbf', 'sigmoid']
for i in range(len(kernels)):
    svc_classifier = SVC(kernel = kernels[i])
    svc_classifier.fit(X_train, y_train)
    svc_scores.append(svc_classifier.score(X_test, y_test))

"""I'll now plot a bar plot of scores for each kernel and see which performed the best.


"""

colors = rainbow(np.linspace(0, 1, len(kernels)))
plt.bar(kernels, svc_scores, color = colors)
for i in range(len(kernels)):
    plt.text(i, svc_scores[i], svc_scores[i])
plt.xlabel('Kernels')
plt.ylabel('Scores')
plt.title('Support Vector Classifier scores for different kernels')

"""The linear kernel performed the best, being slightly better than rbf kernel."""

print("The score for Support Vector Classifier is {}% with {} kernel.".format(svc_scores[0]*100, 'linear'))

"""# Decision tree classifier algoritm
To represent the situation at hand, I'll utilize the Decision Tree Classifier. I'll try a few different max_features to see which one gives the best results.
"""

dt_scores = []
for i in range(1, len(X.columns) + 1):
    dt_classifier = DecisionTreeClassifier(max_features = i, random_state = 0)
    dt_classifier.fit(X_train, y_train)
    dt_scores.append(dt_classifier.score(X_test, y_test))

"""For the split, I chose the maximum amount of characteristics from 1 to 30. Let's look at the results for each of those scenarios.


"""

plt.plot([i for i in range(1, len(X.columns) + 1)], dt_scores, color = 'green')
for i in range(1, len(X.columns) + 1):
    plt.text(i, dt_scores[i-1], (i, dt_scores[i-1]))
plt.xticks([i for i in range(1, len(X.columns) + 1)])
plt.xlabel('Max features')
plt.ylabel('Scores')
plt.title('Decision Tree Classifier scores for different number of maximum features')

"""The model achieved the best accuracy at three values of maximum features, 2, 4 and 18.


"""

print("The score for Decision Tree Classifier is {}% with {} maximum features.".format(dt_scores[17]*100, [2,4,18]))

"""**The score for Decision Tree Classifier is 79.0% with [2, 4, 18] maximum features.**

# Random Forest Classifier
Now I'll design the model using the Random Forest Classifier ensemble approach and change the amount of estimators to observe how they affect the model.
"""

rf_scores = []
estimators = [10, 100, 200, 500, 1000]
for i in estimators:
    rf_classifier = RandomForestClassifier(n_estimators = i, random_state = 0)
    rf_classifier.fit(X_train, y_train)
    rf_scores.append(rf_classifier.score(X_test, y_test))

"""The model has been trained, and the results have been recorded. Let's make a bar graph to compare the results."""

colors = rainbow(np.linspace(0, 1, len(estimators)))
plt.bar([i for i in range(len(estimators))], rf_scores, color = colors, width = 0.8)
for i in range(len(estimators)):
    plt.text(i, rf_scores[i], rf_scores[i])
plt.xticks(ticks = [i for i in range(len(estimators))], labels = [str(estimator) for estimator in estimators])
plt.xlabel('Number of estimators')
plt.ylabel('Scores')
plt.title('Random Forest Classifier scores for different number of estimators')

"""When the total estimators are 100 or 500, the maximum score is obtained."""

print("The score for Random Forest Classifier is {}% with {} estimators.".format(rf_scores[1]*100, [100, 500]))

"""**The score for Random Forest Classifier is 84.0% with [100, 500] estimators.**

# Conclusion
I applied Machine Learning in my research to predict whether or not a person has heart disease. I used plots to analyze the data once it was imported. Then, for category features, I created dummy variables and scaled other features. Then I used K Neighbors Classifier, Support Vector Classifier, Decision Tree Classifier, and Random Forest Classifier, which are all Machine Learning methods. To increase the scores of each model, I changed the parameters. With 8 closest neighbors, K Neighbors Classifier received the maximum score of 87 percent.
"""

